{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Text Classification - AI model for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis** is a common Natural Language Processing (NLP) technique used to determine the emotional tone behind a body of text. It involves in classifying text into pre-defined categories, allowing for gaining insights and monitoring various aspects of business intelligence.\n",
    "\n",
    "This notebook will provide a showcase for a practical application, namely building a model to classify short corpora of text into a label describing the overall emotion/tone. <br />\n",
    "\n",
    "The implementation uses libraries and utilities from [Hugging Face](https://huggingface.co/). For more in-depth information, please refer to the official curriculum.\n",
    "\n",
    "![Hugging Face](./media/hf-logo-with-title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation system that your company is using to gather performance reviews from fellow colleagues is getting old and outdated. Most likely it is written in a language you haven't even heard of. The employees, including yourself, are getting annoying for having to deal with it, and your managers are getting frustrated sorting out the good, and the bad ones. </br >\n",
    "\n",
    "But... Luckily, you have been assigned to lead an initiative to revamp the architecture and, the selling point is, you are **REQUIRED** to use AI for classifying reviews. Some will even go to say that this is the only reason the project was approved by stakeholders. Anyway! This is your task and you **MUST** complete it by the end of sprint.\n",
    "\n",
    "See you in Stand Up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of this workshop is to gain insights and hands on experience with the latest, state-of-the-art tooling for building and fine-tuning AI models. <br />\n",
    "For this purpose, the platform of choice is **Hugging Face**, an open source company that provides a series of performant libraries very popular in the ML space. <br />\n",
    "\n",
    "The model's architecture will start with a pre-trained model (*distilbert-base-uncased*), which will be fine-tuned on a specific dataset. <br />\n",
    "In the following sections, the necessary steps and other basic workflows will be addressed and discussed:\n",
    "- Dependency management and GPU availability \n",
    "- Data discovery and pre-processing \n",
    "- Model architecture and parametrization\n",
    "- Training, analysis and evaluation\n",
    "- Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the required dependencies are installed (if not already satisfied) in the Python environment. Then, the availability of GPU cores is checked and torch is set to use the corresponding backend. <br/>\n",
    "Training and using models on GPU cores is much more faster, as they are optimized for parallel compute.\n",
    "\n",
    "In this notebook, the AI model will run on an architecture build on top of PyTorch. Libraries and tooling from Hugging Face support also Tensorflow. <br />\n",
    "It is important to understand that Hugging Face provides only an unified API for multiple AI [Tasks](https://huggingface.co/tasks), and so, further customization and optimization requires knowledge of at least one of the two popular ML libraries.\n",
    "\n",
    "References:\n",
    "- Check out: [PyTorch](https://pytorch.org/)\n",
    "- Check out: [Tensorflow](https://www.tensorflow.org/)\n",
    "- Check out the Tasks supported by Hugging Face: [Tasks](https://huggingface.co/tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip insatll matplotlib seaborn\n",
    "%pip install datasets transformers torch evaluate scikit-learn sentencepiece protobuf nltk\n",
    "%pip install accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "  mps_device = torch.device('mps')\n",
    "  print('training available on GPU cores')\n",
    "else:\n",
    "  print('training available on CPU cores only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data discovery and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most crucial steps in building AI systems is the selection and the processing of the right amount of data. Datasets are constructed from records\n",
    "consisting of pairs of input (in this case, the *text* or the *query*) and the corresponding label/s. In some cases, datasets are already splitted into subfolders\n",
    "specific for **training**, **testing**, and **evaluation**.\n",
    "\n",
    "`Hugging Face` provides, beside architecture, a multitude of datasets provided and maintained by the community. For the given problem, the `mteb/emotion` dataset.\n",
    "\n",
    "References:\n",
    "- Datasets available on Hugging Face, based on tasks: [Datasets](https://huggingface.co/datasets)\n",
    "- Dataset used in this notebook: [mteb/emotion](https://huggingface.co/datasets/mteb/emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and quick inspection of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the dataset in order to get accustomed with the data available. Datasets from Hugging Face come in different formats and may or may not be ready for further pre-processing steps. <br />\n",
    "\n",
    "Loading, inquirying and altering the dataset can be easily done using the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Declare the official path of the dataset from Hugging Face Datesets\n",
    "dataset_path = 'mteb/emotion' \n",
    "\n",
    "# Load the dataset and check its inner structure\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to inspect the records from the **training**, **testing** or **evaluation** dataset. You can do so by modifying the next line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect different records in the dataset by changing the split type or the index\n",
    "dataset['train'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data discovery through visualisation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some charts in order to visualize the distribution of the labels in the dataset.\n",
    "\n",
    "But first, let's define some mappers between the ids and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the number of labels that exist in the dataset\n",
    "# TODO\n",
    "\n",
    "# Declare a mapping between the ID of the label and the corresponding emotion label\n",
    "# TODO\n",
    "\n",
    "# Declare a mapping between the emotion label and the corresponding ID, programmatically\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the Hugging Face dataset to a Pandas DataFrame\n",
    "# TODO\n",
    "\n",
    "# Create a Bar Plot to visualize the distribution of the emotion labels\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a variable to store the occurences of every emotion label\n",
    "# TODO\n",
    "\n",
    "# Create a Pie Chart to visualize the distribution of the emotion labels in percentage\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column in the data frame to store the length of the text\n",
    "# TODO\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for emotion in df['label'].unique():\n",
    "    sns.histplot(df[df['label'] == emotion]['text_length'], kde=True, label=f\"Emotion {emotion}\", bins=10)\n",
    "\n",
    "plt.title(\"Histogram of Text Length by Emotion\")\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Add the emotion ids as a legend in the visualization\n",
    "# TODO\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_count(text, pos_type):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return len([word for word, pos in pos_tags if pos.startswith(pos_type)])\n",
    "\n",
    "df['noun_count'] = df['text'].apply(lambda x: pos_count(x, 'NN'))  # Nouns\n",
    "df['verb_count'] = df['text'].apply(lambda x: pos_count(x, 'VB'))  # Verbs\n",
    "df['adj_count'] = df['text'].apply(lambda x: pos_count(x, 'JJ'))  # Adjectives\n",
    "df['adv_count'] = df['text'].apply(lambda x: pos_count(x, 'RB'))  # Adverbs\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len([char for char in x if char in string.punctuation]))\n",
    "correlation_matrix = df[['text_length', 'label', 'adj_count', 'adv_count']].corr()\n",
    "\n",
    "# Create a Heat Map to visualize the correlation between the features defined above\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap of Text Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a series of data pre-processing techniques are required in order to prepare the dataset for training.\n",
    "\n",
    "We start by removing the `label_text` modal because it will be irrelevant to the model (which is interested only in the label ids) and because we aim to load into memory as little data as possible. This is crucial for, obviously, performance/time concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the label_text column from the Hugging Face dataset\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define a function that will process a corpus of text in order to simplify the input and to get rid of tokens irrelevant to learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a `preprocess_text` function that pre-processes the raw text,\n",
    "# by mapping the text to lowercase, removing punctuation and numbers,\n",
    "# removing stopwords and applying the lemmatization process.\n",
    "\n",
    "def preprocess_text(text):\n",
    "  raise NotImplementedError(\"Function not implemented\")\n",
    "\n",
    "preprocess_text(\"John did a great job in this quarter, he definetely needs a raise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the fun part *technically* (pun intended) begins. We will start to use different abstractions provided by `Hugging Face` to deal with tokenization and batching techniques.\n",
    "\n",
    "For our given task, the model of choice and the starting point for the fine-tuning process is the `distilbert/distilbert-base-uncased` model - a smaller, but fast version of Google's Bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model path from the Hugging Face Models, and initialize the tokenizer\n",
    "# TODO\n",
    "\n",
    "# Implement a `preprocess_function method needed for the tokenization process`\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tokenizer in place and configured, we can start processing the dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test subsets and apply the tokenization process\n",
    "# Data should be shuffled before applying the tokenization process\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "\n",
    "A Data Collator is a function or class that processes individual samples from a dataset and transforms them into a batch. This involves several tasks, such as:\n",
    "\n",
    "1. **Padding**: Ensuring that all sequences in a batch have the same length by adding padding tokens to shorter sequences. This is essential because models typically require inputs of the same length for batch processing.\n",
    "\n",
    "2. **Tensor Conversion**: Converting lists of inputs (e.g., token IDs, attention masks) into tensors that can be fed into a deep learning model.\n",
    "\n",
    "3. **Mask Creation**: Creating attention masks to distinguish between real tokens and padding tokens.\n",
    "\n",
    "Why? There are multiple benefits of using a Data Collator, but for a high level overview, **efficiency** and **compatibility** are key aspects to follow.\n",
    "\n",
    "- **Efficiency**: Batching samples together is computationally efficient and takes advantage of parallel processing on GPUs.\n",
    "- **Compatibility**: Many models require fixed-size inputs, making padding and other preprocessing steps essential.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Data Collator \n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "**Accuracy** is the proportion of correct predictions among the total number of cases processed. It can be computed with: \n",
    "\n",
    "`Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "Where: \n",
    "- `TP: True positive`\n",
    "- `TN: True negative` \n",
    "- `FP: False positive` \n",
    "- `FN: False negative`\n",
    "\n",
    "**Limitations and Bias**\n",
    "This metric can be easily misleading, especially in the case of unbalanced classes. \n",
    "\n",
    "For example, a high accuracy might be because a model is doing well, but if the data is unbalanced, it might also be because the model is only accurately labeling the high-frequency class. In such cases, a more detailed analysis of the model’s behavior, or the use of a different metric entirely, is necessary to determine how well the model is actually performing.\n",
    "\n",
    "Reference:\n",
    "- sklearn documentation: (accuracy_score)[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This metric, amongst many others, it is available and can be loaded using the `evaluate` library.\n",
    "# This library is a layer of abstration on top of sklearn\n",
    "import evaluate\n",
    "\n",
    "# Also, for custom approaches, the metric can be used directly from the sklearn libary\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Usage with Hugging Face API\n",
    "accuracy_metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "**Precision** is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation: \n",
    "\n",
    "`Precision = TP / (TP + FP)` \n",
    "\n",
    "Where:\n",
    "- `TP is the True positive` \n",
    "- `FP is the False positive `\n",
    "\n",
    "Reference:\n",
    "- sklearn documentation: (precision_score)[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This metric, amongst many others, it is available and can be loaded using the `evaluate` library.\n",
    "# This library is a layer of abstration on top of sklearn\n",
    "import evaluate\n",
    "\n",
    "# Also, for custom approaches, the metric can be used directly from the sklearn libary\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Usage with Hugging Face API\n",
    "precision_metric = evaluate.load('precision')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "**Recall** is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation: \n",
    "\n",
    "`Recall = TP / (TP + FN)`\n",
    "\n",
    " Where:\n",
    "- `TP is the true positives`\n",
    "- `FN is the false negatives`\n",
    "\n",
    "Reference:\n",
    "-  sklearn documentation: [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This metric, amongst many others, it is available and can be loaded using the `evaluate` library.\n",
    "# This library is a layer of abstration on top of sklearn\n",
    "import evaluate\n",
    "\n",
    "# Also, for custom approaches, the metric can be used directly from the sklearn libary\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Usage with Hugging Face API\n",
    "recall_metric = evaluate.load('recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rocauc\n",
    "\n",
    "This metric computes the area under the curve (AUC) for the **Receiver Operating Characteristic Curve (ROC)**. \n",
    "\n",
    "The return values represent how well the model used is predicting the correct classes, based on the input data. \n",
    "\n",
    "A **score of 0.5** means that the model is predicting exactly at chance, i.e. the model’s predictions are correct at the same rate as if the predictions were being decided by the flip of a fair coin or the roll of a fair die. \n",
    "\n",
    "A **score above 0.5** indicates that the model is doing better than chance, while a **score below 0.5** indicates that the model is doing worse than chance.\n",
    "\n",
    "This metric has three separate use cases:\n",
    "\n",
    "- **binary**: The case in which there are only two different label classes, and each example gets only one label. This is the default implementation.\n",
    "- **multiclass**: The case in which there can be more than two different label classes, but each example still gets only one label.\n",
    "- **multilabel**: The case in which there can be more than two different label classes, and each example can have more than one label.\n",
    "\n",
    "Reference:\n",
    "- sklearn documentation: [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This metric, amongst many others, it is available and can be loaded using the `evaluate` library.\n",
    "# This library is a layer of abstration on top of sklearn\n",
    "import evaluate\n",
    "\n",
    "# Also, for custom approaches, the metric can be used directly from the sklearn libary\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Usage with Hugging Face API\n",
    "roc_auc_metric = evaluate.load('roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a custom `compute_metrics` function for computing the accuracy of the model\n",
    "# TODO\n",
    "\n",
    "def compute_metrics(example):\n",
    "  raise NotImplementedError(\"Function not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some basic hyperparameters, together with a brief description and explanation. Of course they are generated with Chat GPT :)\n",
    "\n",
    "`learning_rate`\n",
    "\n",
    "The learning_rate parameter specifies the initial learning rate for the optimizer. \n",
    "\n",
    "It controls how much to change the model weights at each step based on the gradient descent update.\n",
    "\n",
    "A higher learning rate might lead to faster convergence but risks overshooting the optimal point, \n",
    "\n",
    "while a lower learning rate might result in slower convergence and a higher chance of getting stuck in local minima.\n",
    "\n",
    "`num_train_epochs`\n",
    " \n",
    "This parameter indicates the number of complete passes through the training dataset.\n",
    "\n",
    "A higher number might improve learning but could also lead to overfitting.\n",
    "\n",
    "`per_device_train_batch_size`\n",
    "\n",
    "This parameter sets the batch size for training on each device (such as a GPU or CPU).\n",
    "\n",
    "Controls how many samples are processed at once per device during training. A larger batch size might speed up training but requires more memory.\n",
    "\n",
    "`per_device_eval_batch_size`\n",
    "\n",
    "This parameter sets the batch size for evaluation on each device (such as a GPU or CPU).\n",
    "\n",
    "Similar to the training batch size, but used during evaluation (validation) to balance speed and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the pre-trained model used for fine-tuning\n",
    "# TODO\n",
    "\n",
    "# Declare the hyperparameters for the fine-tuning process\n",
    "# TODO\n",
    "\n",
    "# Declare the Trainer and finish the model architecture\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training the model. Be aware that it may take a loooong time, depending on the architecture, the amount of data and the resources available on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model. By running the `evaluate` method, the metrics of the best checkpoint will be computed and displayed (the metric used will be the one specified in the training arguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is defined by running inference on the trained model. This is definetely the easiest and the most fullfiling aspect so far. \n",
    "\n",
    "We are going to use the `pipeline` function from the `transformers` library and use one of the models (checkpoint) saved in the specified directory.\n",
    "\n",
    "It is also possible to save models on the Hugging Face platform (which can be a strategic option for a real life application), but this concern is out of the scope of this workshop.\n",
    "\n",
    "You can feel free to play and test your classifier and be creative in your prompt engineering.\n",
    "\n",
    "Make that model sweat! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the model and check the output\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
